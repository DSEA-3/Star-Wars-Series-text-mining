{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sejjoh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-d76aa616b056>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn=pd.read_csv(\"afinn.csv\",sep='|', encoding='latin-1')\n",
    "nrc=pd.read_csv(\"NRC.csv\")\n",
    "bing=pd.read_csv(\"Bing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep4 =pd.read_table(\"SW_EpisodeIV.txt\")\n",
    "ep5 =pd.read_table(\"SW_EpisodeV.txt\")\n",
    "ep6=pd.read_table(\"SW_EpisodeVI.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                   character \"dialogue\"\\n0     1 \"THREEPIO\" \"Did you hear that?  They\\'ve shut...\\n1                          2 \"THREEPIO\" \"We\\'re doomed!\"\\n2     3 \"THREEPIO\" \"There\\'ll be no escape for the Pr...\\n3                           4 \"THREEPIO\" \"What\\'s that?\"\\n4     5 \"THREEPIO\" \"I should have known better than ...\\n...                                                 ...\\n1005                              1006 \"LUKE\" \"Oh, no!\"\\n1006  1007 \"THREEPIO\" \"Oh, my!  Artoo!  Can you hear...\\n1007  1008 \"TECHNICIAN\" \"We\\'ll get to work on him ri...\\n1008  1009 \"THREEPIO\" \"You must repair him!  Sir, if...\\n1009                  1010 \"LUKE\" \"He\\'ll be all right.\"\\n\\n[1010 rows x 1 columns]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ep4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                  character \"dialogue\"\\n0    1 \"LUKE\" \"Echo Three to Echo Seven. Han, old b...\\n1            2 \"HAN\" \"Loud and clear, kid. What\\'s up?\"\\n2    3 \"LUKE\" \"Well, I finished my circle. I don\\'t ...\\n3    4 \"HAN\" \"There isn\\'t enough life on this ice c...\\n4    5 \"LUKE\" \"Right. I\\'ll see you shortly. There\\'s...\\n..                                                 ...\\n834  835 \"LUKE\" \"I\\'ll meet you at the rendezvous po...\\n835  836 \"LANDO\" \"Princess, we\\'ll find Han. I promi...\\n836  837 \"LUKE\" \"Chewie, I\\'ll be waiting for your s...\\n837  838 \"LUKE\" \"Take care, you two. May the Force ...\\n838                                   839 \"LUKE\" \"Ow!\"\\n\\n[839 rows x 1 columns]'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ep5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                  character \"dialogue\"\\n0    1 \"SHUTTLE CAPTAIN\" \"Command station, this is ...\\n1    2 \"DEATH STAR CONTROLLER\" \"The security deflec...\\n2    3 \"SHUTTLE CAPTAIN\" \"We\\'re starting our approa...\\n3    4 \"OFFICER\" \"Inform the commander that Lord Va...\\n4                             5 \"OPERATOR\" \"Yes, sir.\"\\n..                                                 ...\\n669  670 \"LANDO\" \"Wedge, I don\\'t think we\\'re going ...\\n670  671 \"WEDGE\" \"You\\'ll make it. Just follow me Go...\\n671  672 \"LANDO\" \"I promised to return his ship wit...\\n672                               673 \"HAN\" \"Lando...\"\\n673                      674 \"THREEPIO\" \"They did it!\"\\n\\n[674 rows x 1 columns]'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ep6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function performs cleaning and preprocessing steps to a corpus: str.replace('[^\\w\\s]',''). Remove all punctuation marks str.replace(' ', ''). Remove excess whitespace lambda x: x.lower(). Make all characters lowercase lambda x: ' '.join(item for item in x if item not in new_stopwords_list)). Remove some common English stop words (“I”, “she’ll”, “the”, etc.) str.replace('\\d+', ''). Remove numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleancorpus(txtfile):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # add words that aren't in the NLTK stopwords list\n",
    "    new_stopwords = ['thats','weve','hes','theres','ive','im','will','can','cant','dont','youve','us'\n",
    "        ,'youre','youll','theyre','whats','didnt']\n",
    "    new_stopwords_list = stop_words.union(new_stopwords)\n",
    "    data1 = pd.read_csv(txtfile, delimiter='|')\n",
    "    pd.set_option('max_colwidth', 200)\n",
    "    data1[\"dialogue\"] = data1[\"dialogue\"].str.replace('[^\\w\\s]','')\n",
    "    data1.dialogue = data1.dialogue.apply(lambda x: x.lower())\n",
    "    data1.dialogue = data1.dialogue.str.replace('\\d+', '')\n",
    "    data1.dialogue = data1.dialogue.str.split().apply\\\n",
    "        (lambda x: ' '.join(item for item in x if item not in new_stopwords_list))\n",
    "    data1.dialogue = data1.dialogue.str.replace('  ', '')\n",
    "    #print(data1.dialogue.head(120))\n",
    "    #print(data.dialogue)\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function constructs the term-document matrix, that describes the frequency of terms that occur in a collection of documents. This matrix has terms in the first column and documents across the top as individual column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_n_words(corpus):\n",
    "    word_list = []\n",
    "    dialogue_list = pd.Series(corpus['dialogue'])\n",
    "    dialogue_list_temp = dialogue_list.tolist()\n",
    "    for stat in dialogue_list_temp:\n",
    "        word_list.extend(stat.split())\n",
    "    word_series = pd.Series(word_list)\n",
    "    return word_series.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two functions extract tokens containing two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bigram tokenizer \n",
    "def bigrams_calculate(bigramfile):\n",
    "    i = cleancorpus(bigramfile).dialogue \\\n",
    "        .str.split(expand=True) \\\n",
    "        .stack()\n",
    "    j = i + ' ' + i.shift(-1)\n",
    "    bigrams = j.dropna().reset_index(drop=True)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Most frequent bigrams\n",
    "def most_frequent_bigrams(freq_bigrams):\n",
    "    bigrams_list = pd.Series(freq_bigrams)\n",
    "    count_bigrams = bigrams_list.value_counts().head(20)\n",
    "    return count_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-9fd33697627a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# How many dialogues?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Total Dialogues in Episode 4- A New Hope:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleancorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdialogue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-f694ffd827ff>\u001b[0m in \u001b[0;36mcleancorpus\u001b[1;34m(txtfile)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcleancorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxtfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# add words that aren't in the NLTK stopwords list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     new_stopwords = ['thats','weve','hes','theres','ive','im','will','can','cant','dont','youve','us'\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# How many dialogues?\n",
    "print('Total Dialogues in Episode 4- A New Hope:', len(cleancorpus(ep4).dialogue), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
